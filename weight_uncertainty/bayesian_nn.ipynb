{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from basic import logvariational_fn, samplevariational_fn, logprior_fn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Bayesian layer we have a matrix \n",
    "\n",
    "$\\mathbf{W}$ that is of shape `in_features x out_features`. Each weight $w$ comes from the reparameterized variational Gaussian governed by a deterministic function wrt the parameter $\\mu$ and $\\rho$. So matrices of `in_features x out_features` describe the weight matrix.\n",
    "\n",
    "\n",
    "Pseudocode:\n",
    "\n",
    "- initialize mus, rhos, -- these are the parameters we will change in the optimization. \n",
    "- in a forward pass, we sample the weights given these mus and rhos and use that to do a linear transformation on the input. \n",
    "- meanwhile, we have to calculate the log likelihood wrt to the variational distribution(s) and the log likelihood wrt to the prior over the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesLinear(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Defines a Bayesian Linear Layer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        in_features : int\n",
    "            number of features in the input \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            prior_pi: float,\n",
    "            prior_var1: float,\n",
    "            prior_var2: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Layer attributes\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Parameters governing weights of the layer\n",
    "        # We add a row for biases \n",
    "        self.mus = nn.Parameter(torch.empty(size=(in_features + 1, out_features)).normal_())\n",
    "        self.rhos = nn.Parameter(torch.empty(size=(in_features + 1, out_features)).normal_()) \n",
    "\n",
    "        # Parameters governing weight's prior distribution \n",
    "        self.pi = prior_pi \n",
    "        self.prior_var1 = prior_var1\n",
    "        self.prior_var2 = prior_var2\n",
    "\n",
    "    def __call__(self, x, n_samples):\n",
    "        # For biases\n",
    "        column_of_ones = torch.ones(x.size(0), 1)\n",
    "        x_aug = torch.concat([column_of_ones, x], axis=-1)\n",
    "\n",
    "        sampled_weights = samplevariational_fn(\n",
    "            mus=self.mus,\n",
    "            rhos=self.rhos\n",
    "            )\n",
    "        \n",
    "        # logvariational = logvariational_fn(sampled_weights, self.mus, self.rhos)\n",
    "        # logprior = logprior_fn(sampled_weights, self.pi, self.var1, self.var2)\n",
    "        \n",
    "        return x_aug @ sampled_weights\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "bll = BayesLinear(3, 5, .5, .8, .0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5816,  0.5264,  1.0082],\n",
       "         [-0.6498,  0.4660,  2.0500]]),\n",
       " tensor([[ 1.0000,  0.5816,  0.5264,  1.0082],\n",
       "         [ 1.0000, -0.6498,  0.4660,  2.0500]]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2,3))\n",
    "x_aug = torch.concat([torch.ones((x.size(0), 1)), x], axis=-1)\n",
    "x, x_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7597, -1.2807, -3.2198,  0.8431,  0.1781],\n",
       "        [-2.9015,  7.0670,  1.3891,  0.4946, -1.1932]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bll(x, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3111630757.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[130], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    W_aug =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x_aug = torch.concat([torch.ones((x.size(0), 1)), x], axis=-1)\n",
    "W_aug = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_linear(\n",
    "            in_features: int, \n",
    "            out_features: int, \n",
    "            x, \n",
    "            prior_pi: float,\n",
    "            prior_var1: float,\n",
    "            prior_var2: float, \n",
    "        ):\n",
    "\n",
    "    mus = nn.Parameter(torch.empty(size=(in_features, out_features)).normal_())\n",
    "    rhos = nn.Parameter(torch.empty(size=(in_features, out_features)).normal_())\n",
    "\n",
    "    sampled_weights = samplevariational_fn(\n",
    "        mus=mus,\n",
    "        rhos=rhos\n",
    "        )\n",
    "    \n",
    "    logvariational_prob = logvariational_fn(sampled_weights, mus, rhos)\n",
    "    logprior_prob = logprior_fn(sampled_weights, prior_pi, prior_var1, prior_var2)\n",
    "\n",
    "    return F.linear(x, sampled_weights.T), logvariational_prob.sum(), logprior_prob.sum()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m----> 2\u001b[0m bayes_linear(\n\u001b[1;32m      3\u001b[0m     in_features\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, \n\u001b[1;32m      4\u001b[0m     out_features\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m     x\u001b[39m=\u001b[39;49mx, \n\u001b[1;32m      6\u001b[0m     prior_pi \u001b[39m=\u001b[39;49m \u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     prior_var1\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     prior_var2\u001b[39m=\u001b[39;49m\u001b[39m0.005\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mbayes_linear\u001b[0;34m(in_features, out_features, x, prior_pi, prior_var1, prior_var2)\u001b[0m\n\u001b[1;32m     10\u001b[0m mus \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mempty(size\u001b[39m=\u001b[39m(in_features, out_features))\u001b[39m.\u001b[39mnormal_())\n\u001b[1;32m     11\u001b[0m rhos \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mempty(size\u001b[39m=\u001b[39m(in_features, out_features))\u001b[39m.\u001b[39mnormal_())\n\u001b[0;32m---> 13\u001b[0m sampled_weights \u001b[39m=\u001b[39m samplevariational_fn(\n\u001b[1;32m     14\u001b[0m     mus\u001b[39m=\u001b[39;49mmus,\n\u001b[1;32m     15\u001b[0m     rhos\u001b[39m=\u001b[39;49mrhos\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     18\u001b[0m logvariational_prob \u001b[39m=\u001b[39m logvariational_fn(sampled_weights, mus, rhos)\n\u001b[1;32m     19\u001b[0m logprior_prob \u001b[39m=\u001b[39m logprior_fn(sampled_weights, prior_pi, prior_var1, prior_var2)\n",
      "File \u001b[0;32m~/git/weight_uncertainty/weight_uncertainty/basic.py:147\u001b[0m, in \u001b[0;36msamplevariational_fn\u001b[0;34m(mus, rhos, n_samples)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([mus \u001b[39m+\u001b[39m sigmas \u001b[39m*\u001b[39m epsilons \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples)])\n\u001b[1;32m    146\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m (mus \u001b[39m+\u001b[39m sigmas \u001b[39m*\u001b[39;49m epsilons)\n",
      "File \u001b[0;32m~/git/weight_uncertainty/.venv/lib/python3.11/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x = torch.randn((1, 5), dtype=torch.float)\n",
    "bayes_linear(\n",
    "    in_features=5, \n",
    "    out_features=10, \n",
    "    x=x, \n",
    "    prior_pi = 0.5,\n",
    "    prior_var1=0.7,\n",
    "    prior_var2=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -0.5345, -1.6729, -0.8928, -2.5430, -0.2705],\n",
       "        [ 1.0000, -0.7756,  1.6903,  0.6987, -1.0473, -0.1190],\n",
       "        [ 1.0000,  0.5254, -1.2813,  0.6321,  0.2138, -0.9132]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((3,5))\n",
    "W_aug = torch.concat([torch.ones(3,1), W], axis=-1)\n",
    "W_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
