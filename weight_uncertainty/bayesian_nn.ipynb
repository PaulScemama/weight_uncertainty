{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmas_from_rhos(rhos: Float[Tensor, \"N D\"]) -> Float[Tensor, \"N D\"]:\n",
    "    return torch.log(1 + torch.exp(rhos))\n",
    "\n",
    "\n",
    "def logvariational_fn(\n",
    "    weights: Float[Tensor, \"N D\"],\n",
    "    mus: Float[Tensor, \"N D\"],\n",
    "    sigmas: Float[Tensor, \"N D\"],\n",
    ") -> Float[Tensor, \"*N 1\"]:\n",
    "    \"\"\"\n",
    "    Computes the log density of a diagonal multivariate gaussian distribution given\n",
    "    the parameters governing the distribution (mean_vetors, sigmas) and an\n",
    "    input weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        weights : N x D tensor\n",
    "            N D-dimensional vectors representing the weights of the network to be\n",
    "            evaluated.\n",
    "        mus : N x D tensor\n",
    "            N D-dimensional vectors representing the mean vectors for the\n",
    "            N independent multivariate gaussian distributions to evaluate\n",
    "            weights under.\n",
    "        sigmas : N x D tensor\n",
    "            N D-dimensional vectors representing the diagonal entries of the covariance\n",
    "            matrix for N independent multivariate gaussian distributions to evaluate\n",
    "            weights under.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    N x 1 tensor\n",
    "        the log density for each N weight vectors according to the parameters in\n",
    "        mus and sigmas.\n",
    "    \"\"\"\n",
    "    # If a 0 is in sigmas -> non-singular since sigmas are diagonals\n",
    "    if not sigmas.all():\n",
    "        raise ValueError(f\"sigmas need to all be positive, but they are {sigmas}\")\n",
    "    # this is from Daniel W https://stackoverflow.com/questions/48686934/numpy-vectorization-of-multivariate-normal\n",
    "    D = weights.size(1)\n",
    "    constant = D * np.log(2 * torch.pi)\n",
    "    log_determinants = torch.log(torch.prod(sigmas, axis=1)) \n",
    "    deviations = weights - mus\n",
    "    inverses = 1 / sigmas\n",
    "    return -0.5 * (constant + log_determinants +\n",
    "        torch.sum(deviations * inverses * deviations, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "def samplevariational_fn(\n",
    "    n_samples: int,\n",
    "    mus: Float[Tensor, \"N D\"],\n",
    "    sigmas: Float[Tensor, \"N D\"],\n",
    "    epsilons: Float[Tensor, \"N D\"],\n",
    ") -> Float[Tensor, \"N D\"]:\n",
    "    \"\"\"\n",
    "    Samples from a diagonal multivariate gaussian governed by the mus\n",
    "    and sigmas.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        n_samples : int\n",
    "            number of samples to return.\n",
    "        mus : N x D tensor\n",
    "            N D-dimensional vectors representing the mean vectors for N independent\n",
    "            multivariate gaussian distributions.\n",
    "        sigmas : N x D tensor\n",
    "            N D-dimensional vectors representing the diagonal entries of the covariance\n",
    "            matrix for N independent multivariate gaussian distributions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    n_samples x N x D tensor\n",
    "        n_samples of the N x D tensor that represents N samples from N independent\n",
    "        D-dimensional multivariate gaussian distributions.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for _ in range(n_samples):\n",
    "        samples.append(mus + sigmas * epsilons)\n",
    "    return torch.stack(samples)\n",
    "\n",
    "\n",
    "def logprior_fn(weights: Float[Tensor, \"N D\"], pi: float, sigma1: float, sigma2: float):\n",
    "    print(weights.shape)\n",
    "\n",
    "    gaussian1_log_prob = stats.multivariate_normal.logpdf(\n",
    "        x=weights, mean=0, cov=sigma1**2\n",
    "    )\n",
    "    gaussian2_log_prob = stats.multivariate_normal.logpdf(\n",
    "        x=weights, mean=0, cov=sigma2**2\n",
    "    )\n",
    "\n",
    "    return torch.log(\n",
    "        pi * torch.exp(gaussian1_log_prob) + (1 - pi) * torch.exp(gaussian2_log_prob)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones((2,3))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mus = torch.ones((2,3)) * 3\n",
    "mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas = torch.ones((2,3))\n",
    "sigmas = torch.arange(1,7).reshape(2,3)\n",
    "sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.3194, -6.3839])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logvariational_fn(weights, mus, sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.]), tensor([3., 3., 3.]), tensor([1, 2, 3]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0], mus[0], sigmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.319362000894712"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.multivariate_normal.logpdf(weights[0], mus[0], sigmas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.383894804338374"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.multivariate_normal.logpdf(weights[1], mus[1], sigmas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
